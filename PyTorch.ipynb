{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rKceRbV7H0GX",
        "3OOT3HtpJKIz",
        "56cFWavdz0jB",
        "BjLcHG2nNUuL",
        "tSIuq2pRWKw_",
        "XYqg0A-VAGbv",
        "Tsu1McKNB4_t",
        "bQAZGD5VEzJy",
        "EB0WM8wQHoIC",
        "_4aC2H5SJ5D3",
        "U8yojqARM7Pz",
        "m8Kt_tXMPCj_",
        "-u3nAsNOkQ90",
        "eTPzTzkJlyUp",
        "Ni09RrrJmiFH",
        "xMdM4Ak0ndP8",
        "hHHIBwCcn_5o",
        "tLzl19TnbaLz",
        "zzoLLs6-hljn"
      ],
      "toc_visible": true,
      "mount_file_id": "111WLcYwXtQS7Ac57T_37t-et_oAMvbxS",
      "authorship_tag": "ABX9TyNQNBd4c6p4enAJKg07V+Ab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickmsshin/PyTorch/blob/main/PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKceRbV7H0GX"
      },
      "source": [
        "## Curriculum\n",
        "[Reference1]\n",
        "https://www.youtube.com/watch?time_continue=1&v=GIsg-ZUy0MY&feature=emb_logo\n",
        "\n",
        "[Refreence2]\n",
        "https://www.youtube.com/playlist?list=PLWKjhJtqVAbm5dir5TLEy2aZQMG7cHEZp\n",
        "\n",
        "1. PyTorch Basics: Tensor & Gradients\n",
        "2. Linear Regression & Gradient Descent\n",
        "3. Image Classification using Ligistic Regression\n",
        "4. Training Deep Neural Network on a GPU\n",
        "5. CNN, Regularization and ResNets\n",
        "6. Generative Adverserial Netwokrs(GAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OOT3HtpJKIz"
      },
      "source": [
        "# 1.PyTorch Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56cFWavdz0jB"
      },
      "source": [
        "##1.1.Tensors\n",
        "* Tensors: PyTorch is a library for processing tensors. A tensor is a number, vector, matix or any n-dimensional array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfbtdDuEGv16"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRuG7FSJJoE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12af2ebd-f07a-4c2b-a199-d1962e609f51"
      },
      "source": [
        "# Create a tensor with a single number\n",
        "t1 = torch.tensor(4.)  # 4. = 4.0\n",
        "t1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFT11wOtAxui",
        "outputId": "340e6f57-c09c-409a-c713-2b68a27c8a2d"
      },
      "source": [
        "t1.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhbdXCTyJ9rg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ef9b9e-84b3-41f0-bd6a-8dfc34f6bdda"
      },
      "source": [
        "# Create a slightly more complex tensor\n",
        "# Vector \n",
        "t2 = torch.tensor([1.,2,3,4]) # size = [4]\n",
        "t2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3., 4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amg8Q8l3B-uG"
      },
      "source": [
        "tensor 안에 한 값이 float값이면, 다른 값들의 dtype도 float로 변경된다.\n",
        "  \n",
        "ex) 1. -> float\n",
        "    2,3,4 -> int "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZxPZ091BDPY",
        "outputId": "a049d070-550c-45fb-8fbb-fefaa6d9c071"
      },
      "source": [
        "# Matrix\n",
        "t3 = torch.tensor([[5., 6.],\n",
        "                   [7., 8.],\n",
        "                   [9, 10]])  # shape -> size([3, 2])\n",
        "t3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.,  6.],\n",
              "        [ 7.,  8.],\n",
              "        [ 9., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn1a8QEKBK86",
        "outputId": "aee069ca-aa40-4a0d-a7d1-8b821350d77a"
      },
      "source": [
        "# 3-dimensional array\n",
        "t4 = torch.tensor([[[1.,2,3],\n",
        "                   [5, 6,4]],\n",
        "                   [[8.,9,4],\n",
        "                   [12, 13,4]],\n",
        "                   [[8.,9,4],\n",
        "                   [12, 13,4]],\n",
        "                   [[8.,9,4],\n",
        "                   [12, 13,4]]])  # shape -> size([4, 2, 3]) depths, rows, columns\n",
        "t4.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpWapK-3E2pn",
        "outputId": "79e5e90a-8a5d-4a6a-92cf-cd4c4234525c"
      },
      "source": [
        "t4 = torch.tensor([[[1.,2.,5,6]\n",
        "\t                 ,[3,4.,7,8]\n",
        "\t\t\t\t\t\t\t\t\t ,[5, 6, 9, 10]],\n",
        "\t\t\t\t\t\t\t\t\t [[1.,2., 3,9]\n",
        "                    ,[3, 4,12,11]\n",
        "\t\t\t\t\t\t\t\t\t\t,[5, 6,7,9.]]])\n",
        "\n",
        "t4.shape  # size([2,3,4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_1te9bKKEU9"
      },
      "source": [
        "# Tensor should have a regular shape\n",
        "# t5 will show error, since the last element has different size with other elements\n",
        "#t5 = torch.tensor([[5.,6.], [7.,8.], [9., 10., 11.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjLcHG2nNUuL"
      },
      "source": [
        "##1.2.Tensor operations and gradients\n",
        "- We can combine tensors with the usual arithmetic operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUhj91XGNaST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb1773cf-5de6-46ef-c3eb-4de471760599"
      },
      "source": [
        "# Create tensors\n",
        "x = torch.tensor(3.)                      # 우리는 x 미분에는 관심없다.\n",
        "w = torch.tensor(4., requires_grad=True)\n",
        "b = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "x, w, b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx6Ph6-2ON5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c056baea-a0ad-48f4-f900-1b08e572213e"
      },
      "source": [
        "# Arithmetic operations\n",
        "y = w * x + b\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17., grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2I7Zj4xOw6D"
      },
      "source": [
        "- What makes PyTorch speical is that we can automatically compute the derivative of 'y' w.r.t the tensors that have \"requires_grad\" set to \"True\".\n",
        "- To compute the derivatives, we can call the \".backward\" method on our output function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8M_hrrGOnh1"
      },
      "source": [
        "# Compute Derivatives\n",
        "# requires_grad = True 로 설정해준 모든 input variable에 대해서 미분 수행. \n",
        "# w, b 모두에 대해서 각각 미분 수행\n",
        "# 모든 input variable 에 대해서 미분 수행시 비효율 증가.(나중에 변수 개수 많아짐.)\n",
        "y.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pm0g7PPPx-4"
      },
      "source": [
        "* The derivatives of 'y' w.r.t. the input tensors are stored in the '.grad' property of the respective tensors. (requires_grad = True 로 설정했던 각각의 입력 텐서에 대한 'y'의 미분은 각 텐서의 '.grad'에 저장됩니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyC7MVaQPgUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b4353bb-3e3f-4fc9-8dd6-748ca0c195c4"
      },
      "source": [
        "print('dy/dx', x.grad)  # requires_grad 옵션을 주지 않았기 때문에 미분 미수행 : none\n",
        "print('dy/dw', w.grad)  # y = w*3 + 5 로 보고 미분 수행 \n",
        "print('dy/db', b.grad)  # y = 4*3 + b 로 보고 미분 수행"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dy/dx None\n",
            "dy/dw tensor(3.)\n",
            "dy/db tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3sMVZq6LqNS"
      },
      "source": [
        "##1.3.Tensor functions\n",
        "- Apart from arithmetic operations, the torch module also contains many functions for creating and manipulating tensors. Let's look at some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v65CHj3wL_rM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f772b3-b1fb-4d8a-c2f3-7d09b2f4902c"
      },
      "source": [
        "# Create a tensor with a fixed value for every element\n",
        "# 모두 동일한 값을 갖는 tensor 생성\n",
        "t6 = torch.full((3,2), 42)\n",
        "t6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[42, 42],\n",
              "        [42, 42],\n",
              "        [42, 42]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuNNlAMgMVgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15147444-dd26-4c59-baa6-0c06fcf0593f"
      },
      "source": [
        "# Concatenate two tensors with compatible shapes\n",
        "t7 = torch.cat((t3, t6)) # 3x2 , 3x2  -> size([6,2])\n",
        "t7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.,  6.],\n",
              "        [ 7.,  8.],\n",
              "        [ 9., 10.],\n",
              "        [42., 42.],\n",
              "        [42., 42.],\n",
              "        [42., 42.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXFQd19M8Cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688fd0bc-1d44-41bb-d5c9-ca29636782fb"
      },
      "source": [
        "# Compute the sin of each element\n",
        "# tensor에 있는 각 element에 'sin' 연산 수행\n",
        "t8 = torch.sin(t7)\n",
        "t8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9589, -0.2794],\n",
              "        [ 0.6570,  0.9894],\n",
              "        [ 0.4121, -0.5440],\n",
              "        [-0.9165, -0.9165],\n",
              "        [-0.9165, -0.9165],\n",
              "        [-0.9165, -0.9165]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01DkSJNZNXyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bacefb2c-c3e7-48f8-f468-95f0e1f54b20"
      },
      "source": [
        "# Chage the shape of a tensor\n",
        "t9 = t8.reshape(3, 2, 2)   # t8 -> size 6x2 = 12 elements\n",
        "t9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.9589, -0.2794],\n",
              "         [ 0.6570,  0.9894]],\n",
              "\n",
              "        [[ 0.4121, -0.5440],\n",
              "         [-0.9165, -0.9165]],\n",
              "\n",
              "        [[-0.9165, -0.9165],\n",
              "         [-0.9165, -0.9165]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8zD9_j1OPhI"
      },
      "source": [
        "You can learn more about tensor operations here: https://pytorch.org/docs/stable/torch.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaeLqy-MPmZN"
      },
      "source": [
        "##1.4.Interoperability with Numpy\n",
        "Numpy is a popular open-source library used for mathematical and scientific computing in Python. It enables efficient operations on large multi-dimensional arrays and has a vast ecosystem of supporting libraries, including\n",
        "- Pandas for file I/O and data analysis\n",
        "- Matplotlib for plotting and visualization\n",
        "- OpenCV for image and video processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCpnDNVDQOpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c094f0ea-e36a-4cd4-a96d-c70bd9584324"
      },
      "source": [
        "# Create an array in Numpy\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[1, 2], [3,4.]])\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2.],\n",
              "       [3., 4.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa0ffOI_8CsI"
      },
      "source": [
        "We can convert a Numpy array to PyTorch tensor using torch.from_numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfWlusc8J2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206a20a3-68ac-44b3-a0ed-52b5e1a483ea"
      },
      "source": [
        "import torch\n",
        "\n",
        "y = torch.from_numpy(x)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGhNPHNM8XRS"
      },
      "source": [
        "The numpy array and torch tensor have similar data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cSrFza48WX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0edb1f-2175-4212-e213-66f00e3768b1"
      },
      "source": [
        "x.dtype, y.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dtype('float64'), torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ1v-7rc8kF5"
      },
      "source": [
        "We can convert PyTorch tensor to a Numpy array using the .numpy method of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap2YiX3E8spA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4c2b69-2375-4b38-ca63-ba7f00a085fe"
      },
      "source": [
        "z = y.numpy()\n",
        "z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2.],\n",
              "       [3., 4.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAlUGdS586Kn"
      },
      "source": [
        "The interoperability between PyTorch and Numpy is essential because most datasets you'll work with will likely be read and preprocessed as Numpy arrays.\n",
        "(대부분의 입력 데이터와 데이터 전처리들이 Numpy array 형태로 진행이 될 것이기 때문에 PyTorch와 Numpy를 함께 잘 사용할 수 있어야 합니다.)\n",
        "\n",
        "You might wonder why we neea a library like PyTorch like PyTorch at all since Numpy already provides data structures and utilities for working with multi-dimensional numeric data. There are two main reasons,\n",
        "\n",
        "1. AutoGrad: The ability to automaticaaly compute gradients for tensor operations is essential for training deep learning models. \n",
        "2. GPU support: While working with massive datasets and large models, PyTorch tensor operations can be performed efficiently using Graphical Processing Units(GPU).Computations that might typically take hourse can be completed within minutes using GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSIuq2pRWKw_"
      },
      "source": [
        "#2.Gradient Descent and Linear Regression with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYqg0A-VAGbv"
      },
      "source": [
        "##2.1.Introduction to Linear Regression\n",
        "In this tutorial, we'll discuss one of the foundational algorithms in machine learning: Linear regression. We'll create a model that predicts crop yields for apples and oranges (target variables) by looking at the average temperature, rainfall, and humidity (input variables or features) in a region. Here's the training data:\n",
        "\n",
        "<p align=\"center\"><img src=\"https:%5C%5Cdrive.google.com%5Cuc?export=view&id=1E_YIReAANKaFNbSiiQFQwuZyYiFpJi7I&raw=1\" width=\"60%\">\n",
        "\n",
        "<p align=\"center\"><img src=\"https://drive.google.com/uc?export=view&id=1E_YIReAANKaFNbSiiQFQwuZyYiFpJi7I\" width=\"60%\">\n",
        "\n",
        "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
        "\n",
        "```\n",
        "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
        "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YG3yhJ-bH2i"
      },
      "source": [
        "The learning part of linear regression is to figure out a set of weights w11, w12,... w23, b1 & b2 using the training data, to make accurate predictions for new data. The learned weights will be used to predict the yields for apples and oranges in a new region using the average temperature, rainfall, and humidity for that region.\n",
        "\n",
        "We'll train our model by adjusting the weights slightly many times to make better predictions, using an `optimization technique called gradient descent.`Let's begin by importing Numpy and PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOTxcEwSAEld"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsu1McKNB4_t"
      },
      "source": [
        "##2.2.Training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I73nHo7LbpWU"
      },
      "source": [
        "We can represent the training data using two matrices: `inputs` and `targets`, each with one row per observation, and one column per variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rsCAmbPB_Yb"
      },
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43], \n",
        "                   [91, 88, 64], \n",
        "                   [87, 134, 58], \n",
        "                   [102, 43, 37], \n",
        "                   [69, 96, 70]], dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lZbpZopED_3"
      },
      "source": [
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56, 70], \n",
        "                    [81, 101], \n",
        "                    [119, 133], \n",
        "                    [22, 37], \n",
        "                    [103, 119]], dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nfXRCs-cYg9"
      },
      "source": [
        "We've separated the input and target variables because we'll operate on them separately. Also, we've created numpy arrays, because this is typically how you would work with training data: \n",
        "1. Read some CSV files as numpy arrays.\n",
        "2. Do some processing.\n",
        "3. And then convert them to PyTorch tensors.\n",
        "\n",
        "Let's convert the arrays to PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyU0PZf2Ea7o"
      },
      "source": [
        "# Convert Numpy array to PyTorch tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HveEvWlhErhw"
      },
      "source": [
        "print(inputs)\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAq9s2E5YA3k"
      },
      "source": [
        "##2.3.Linear regression model from scratch\n",
        "The weights and biases (`w11, w12,... w23, b1 & b2`) can also be represented as matrices, initialized as random values. The first row of `w` and the first element of `b` are used to predict the first target variable, i.e., yield of apples, and similarly, the second for oranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCrWeMifd-Qu"
      },
      "source": [
        "# Define initial weight and bias\n",
        "w = torch.randn(2,3, requires_grad=True)\n",
        "b = torch.randn(1,2, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVxxqq-JfjGs"
      },
      "source": [
        "`torch.randn` creates a tensor with the given shape, with elements picked randomly from a normal distribution with mean 0 and standard deviation 1.\n",
        "\n",
        "Our model is simply a function that performs a matrix multiplication of the `inputs` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Y = X  W.t + B\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "We can define the model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg6dlF1qgQK8"
      },
      "source": [
        "# Linear regression model\n",
        "def model(x):\n",
        "  return x @ w.t() + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIVMs5efgm71"
      },
      "source": [
        "`@` represents matrix multiplication in PyTorch, and the `.t` method returns the transpose of a tensor.\n",
        "\n",
        "The matrix obtained by passing the input data into the model is a set of predictions for the target variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpvzffHqgq4I"
      },
      "source": [
        "# Generate predictions\n",
        "preds= model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RySBWN_LhGF0"
      },
      "source": [
        "Let's compute the predictions of our model with the actual targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x95iHFuQhMWh"
      },
      "source": [
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcCvwdF4hPA2"
      },
      "source": [
        "You can see a big difference between our model's predictions and the actual targets because we've initialized our model with random weights and biases. Obviously, we can't expect a randomly initialized model to just work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQAZGD5VEzJy"
      },
      "source": [
        "##2.4.Loss Function\n",
        "[참고]\n",
        "https://www.youtube.com/watch?v=TxIVr-nk1so&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=6\n",
        "\n",
        "- Before we improve our model, we need a way to evaluate how well our model is performing. We can compare the model's predictions with the actual targets, using the following method:\n",
        "\n",
        "  - Calculate the difference between the two matrices (`preds` and `targets`).\n",
        "  - Square all elements of the difference matrix to remove negative values.\n",
        "  - Calculate the average of the elements in the resulting matrix.\n",
        "\n",
        "- The result is a single number, known as the mean squared error (MSE).\n",
        "\n",
        "\n",
        "<p align=\"center\"><img src=\"https://drive.google.com/uc?export=view&id=1qJf62D5EGVgJi3xD0JGzk433CZz9A1Ge\" width=\"40%\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naceMyYpFVpQ"
      },
      "source": [
        "# Define Loss Function: MSE\n",
        "def mse(t1, t2):\n",
        "  diff = t1 - t2\n",
        "  return torch.sum(diff*diff) / diff.numel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Or5nt2ih9R_"
      },
      "source": [
        "`torch.sum` returns the sum of all the elements in a tensor. The `.numel` method of a tensor returns the number of elements in a tensor. Let's compute the mean squared error for the current predictions of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEPwZ6LriIeD"
      },
      "source": [
        "# Compute the mse\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHgsUv4liUxy"
      },
      "source": [
        "Here’s how we can interpret the result: On average, each element in the prediction differs from the actual target by the square root of the loss. And that’s pretty bad, considering the numbers we are trying to predict are themselves in the range 50–200. The result is called the loss because it indicates how bad the model is at predicting the target variables. It represents information loss in the model: `the lower the loss, the better the model`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB0WM8wQHoIC"
      },
      "source": [
        "##2.5.Compute Gradients\n",
        "- With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases, because they have requires_grad set to True.\n",
        "\n",
        "* .backward()는 requires_grad로 설정한 모든 변수(ex. w, b)에 대해서 loss의 미분식을 구한 후 앞서 정의 된 w, b 값을 대입하여 최종 미분값을 전부 구해주는 기능이다. \n",
        "\n",
        "* 변경사항 저장 되는지 확인하기.\n",
        "\n",
        "[계산 과정 설명]\n",
        "\n",
        "https://www.youtube.com/watch?v=ma2KXWblllc&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPusgNFtH3U4"
      },
      "source": [
        "# Compute Gradients\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnq1iL9_H-Df"
      },
      "source": [
        "The gradients are stored in the '.grad' property of the respective tensors. Note that the derivative of the loss w.r.t. the weights matrix is itself a matrix, with the same dimensions.\n",
        "\n",
        ".backward()로 구해진 gradient 값들은 전부 `.grad`에 저장이 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzBOlI4QILJm"
      },
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_OfUXqrtwBs"
      },
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4aC2H5SJ5D3"
      },
      "source": [
        "##2.6.Adjust weights and biases to reduce the loss\n",
        "\n",
        "[참고: Sung Kim]\n",
        "https://www.youtube.com/watch?v=b4Vyma9wPHo&list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&index=3\n",
        "\n",
        "[Other Ref.]https://computer-nerd.tistory.com/5\n",
        "\n",
        "The loss is a quadratic function of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss w.r.t any individual weight or bias element, it will look like the figure shown below. An important insight from calculus is that the gradient indicates the rate of change of the loss, i.e., the loss function's slope w.r.t. the weights and biases.\n",
        "\n",
        "If a gradient element is `positive`:\n",
        "\n",
        "* `increasing` the weight element's value slightly will `increase` the loss\n",
        "* `decreasing` the weight element's value slightly will `decrease` the loss\n",
        "\n",
        "<p align=\"center\"><img src=\"https://drive.google.com/uc?export=view&id=1lpYTykep-CW0cx13O062Mw5hikdl8m3B\" width=\"50%\">\n",
        "\n",
        "\n",
        "If a gradient element is `negative`:\n",
        "* `increasing` the weight element's value slightly will `decrease` the loss\n",
        "* `decreasing` the weight element's value slightly will `increase` the loss\n",
        "\n",
        "<p align=\"center\"><img src=\"https://drive.google.com/uc?export=view&id=1Eq2LWUxbjzna9ocyFle1mBlYWuA09dh6\" width=\"50%\">\n",
        "\n",
        "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss w.r.t. that element. This observation forms the basis of the gradient descent optimization algorithm that we'll use to improve our model (by descending along the gradient).\n",
        "\n",
        "We can subtract from each weight element a small quantity proportional to the derivative of the loss w.r.t. that element to reduce the loss slightly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjvA4LNfKTb-"
      },
      "source": [
        "# 1. Prediction\n",
        "preds = mod_linear(ttInput)\n",
        "print(preds)\n",
        "\n",
        "# 2. Calculate the loss\n",
        "loss = mse(preds, ttTargets)\n",
        "print(loss)\n",
        "\n",
        "# 3. Compute gradients w.r.t the weights and biases\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJDhWtrMLiKI"
      },
      "source": [
        "Finally, we update the weights and biases using the gradients computed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ1Tye1lKnTC"
      },
      "source": [
        "# 4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "# 5. Reset the gradients to zero\n",
        "\n",
        "# Adjust weights & reset gradients\n",
        "with torch.no_grad():\n",
        "    # Step 4. \n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5        \n",
        "    # Step 5.\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4u5WnKHL2u4"
      },
      "source": [
        "A few things to note above:\n",
        "\n",
        "- We use torch.no_grad to indicate to PyTorch that we shouldn't track, calculate or modify gradients while updating the weights and biases.\n",
        "\n",
        "- We multiply the gradients with a really small number (10^-5 in this case), to ensure that we don't modify the weights by a really large amount, since we only want to take a small step in the downhill direction of the gradient. This number is called the learning rate of the algorithm.\n",
        "\n",
        "- After we have updated the weights, we reset the gradients back to zero, to avoid affecting any future computations.\n",
        "\n",
        "Let's take a look at the new weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owDTMOFuL75N"
      },
      "source": [
        "# New Weight\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7Qe-kXUMaq6"
      },
      "source": [
        "# Calculate loss\n",
        "preds = mod_linear(ttInput)\n",
        "loss = mse(preds, ttTargets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Xrq-eOSWqX"
      },
      "source": [
        "##2.7.Train the model using gradient descent\n",
        "As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm, which has the following steps:\n",
        "\n",
        "1. Generate predictions\n",
        "2. Calculate the loss\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "5. Reset the gradients to zero\n",
        "\n",
        "Let's implement the above step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8yojqARM7Pz"
      },
      "source": [
        "##2.8.Train for multiple epochs\n",
        "To reduce the loss further, we can repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch. Let's train the model for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJkXviwcNDdL"
      },
      "source": [
        "# Train model for 100 epochs\n",
        "for i in range(10000):\n",
        "  preds = mod_linear(ttInput)\n",
        "  loss = mse(preds, ttTargets)\n",
        "  loss.backward\n",
        "  with torch.no_grad():\n",
        "    # Step 4. \n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5        \n",
        "    # Step 5.\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  if i%1000 == 0:\n",
        "    print(\"Epoch:\", i, \"Loss:\", loss)\n",
        "  if loss < 100: break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIL9jP_4OUxE"
      },
      "source": [
        "# Calculate loss\n",
        "preds = mod_linear(ttInput)\n",
        "loss = mse(preds, ttTargets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0dOTjhvOfxz"
      },
      "source": [
        "print(preds)\n",
        "print(ttTargets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Kt_tXMPCj_"
      },
      "source": [
        "##2.8.Linear regression using PyTorch built-ins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlptUJ4XjpEl"
      },
      "source": [
        "The model and training process above were implemented using basic matrix operations. But since this such a common pattern , PyTorch has several built-in functions and classes to make it easy to create and train models.\n",
        "\n",
        "Let's begin by importing the torch.nn package from PyTorch, which contains utility classes for building neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV5Jx8unjoJo"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5FAwg0Wj7SV"
      },
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
        "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
        "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
        "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
        "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
        "                  dtype='float32')\n",
        "\n",
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56, 70], [81, 101], [119, 133], \n",
        "                    [22, 37], [103, 119], [56, 70], \n",
        "                    [81, 101], [119, 133], [22, 37], \n",
        "                    [103, 119], [56, 70], [81, 101], \n",
        "                    [119, 133], [22, 37], [103, 119]], \n",
        "                   dtype='float32')\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u3nAsNOkQ90"
      },
      "source": [
        "##2.9.Dataset and DataLoader\n",
        "We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn135fNgkeIW"
      },
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBCkr1R7k8Zv"
      },
      "source": [
        "# Define dataset\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CykzV7Wckoeu"
      },
      "source": [
        "# Define DataLoader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB1FiUx6lbDO"
      },
      "source": [
        "The data loader is typically used in a for-in loop. Let's look at an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOyOB7tflchg"
      },
      "source": [
        "for xb, yb in train_dl:\n",
        "    print(xb)\n",
        "    print(yb)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsmMtIsblnF3"
      },
      "source": [
        "in each iteration, the data loader returns one batch of data, with the given batch size. If shuffle is set to True, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, which can lead to faster reduction in the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTPzTzkJlyUp"
      },
      "source": [
        "##2.10.nn.Linear\n",
        "Instead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDXyFAWClsBb"
      },
      "source": [
        "model = nn.Linear(3,2)\n",
        "print(model.weight)\n",
        "print(model.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxHEQ8anmQU9"
      },
      "source": [
        "We can use the model to generate predictions in the exact same way as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyACW-0mRLv"
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni09RrrJmiFH"
      },
      "source": [
        "##2.11.Loss Function\n",
        "Instead of defining a loss function manually, we can use the built-in loss function mse_loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_6uS3ShmsWo"
      },
      "source": [
        "# Import nn.functional\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39yWod7zm4n_"
      },
      "source": [
        "The nn.functional package contains many useful loss functions and several other utilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snI38BPKm8NP"
      },
      "source": [
        "# Define Loss Function\n",
        "loss_fn = F.mse_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GfU5LpXnJxd"
      },
      "source": [
        "# Compute the loss for the current predictions of our model.\n",
        "loss = loss_fn(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMdM4Ak0ndP8"
      },
      "source": [
        "##2.12.Optimizer\n",
        "Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer optim.SGD. SGD stands for stochastic gradient descent. It is called stochastic because samples are selected in batches (often with random shuffling) instead of as a single group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnF8r7DTnjLo"
      },
      "source": [
        "# Define Optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOVGcK1Bn0me"
      },
      "source": [
        "Note that model.parameters() is passed as an argument to optim.SGD, so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate which controls the amount by which the parameters are modified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHHIBwCcn_5o"
      },
      "source": [
        "##2.13.Train the model\n",
        "We are now ready to train the model. We'll follow the exact same process to implement gradient descent:\n",
        "\n",
        "1. Generate predictions\n",
        "2. Calculate the loss\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "5. Reset the gradients to zero\n",
        "\n",
        "The only change is that we'll work batches of data, instead of processing the entire training data in every iteration. Let's define a utility function fit which trains the model for a given number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4knx5XyoLB2"
      },
      "source": [
        "# Utility function to train the model\n",
        "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
        "\n",
        "  # Repeat for given number of epoch\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Train with batches of data:\n",
        "    for xb, yb in train_dl:\n",
        "\n",
        "      # 1. Generate predictions\n",
        "      preds = model(xb)\n",
        "\n",
        "      # 2. Calculate the loss\n",
        "      loss = loss_fn(preds, yb)\n",
        "      \n",
        "      # 3. Compute gradients w.r.t the weights and biases\n",
        "      loss.backward()\n",
        "\n",
        "      # 4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "      opt.step()\n",
        "\n",
        "      # 5. Reset the gradients to zero\n",
        "      opt.zero_grad()\n",
        "\n",
        "    # Print parameters\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARszIVXvpoSW"
      },
      "source": [
        "Some things to note above:\n",
        "\n",
        "- We use the data loader defined earlier to get batches of data for every iteration.\n",
        "- Instead of updating parameters (weights and biases) manually, we use opt.step to perform the update, and opt.zero_grad to reset the gradients to zero.\n",
        "- We've also added a log statement which prints the loss from the last batch of data for every 10th epoch, to track the progress of training. loss.item returns the actual value stored in the loss tensor.\n",
        "\n",
        "Let's train the model for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSNr4X9Kprt6"
      },
      "source": [
        "# Generate a trained model(fit)\n",
        "fit(100, model, loss_fn, opt, train_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHfc8t7Ep6Hn"
      },
      "source": [
        "# Generate Prediction\n",
        "final_pred = model(inputs)\n",
        "print(final_pred)\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTmaPYxysjOY"
      },
      "source": [
        "#3.Image Classification using Logistic Regression in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POd_n4IqVSH4"
      },
      "source": [
        "### 3.1.Exploring the Data\n",
        "We begin by importing torch and torchvision. torchvision contains some utilities for working with image data. It also contains helper classes to automatically download and import popular datasets like MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKkI-Dp-VXc8"
      },
      "source": [
        "# Import library\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5IDnSCnVtUM"
      },
      "source": [
        "# Download dataset\n",
        "dataset = MNIST(root='MNIST/', download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGLEOPoWWSLI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6S54IVHXJVi"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-CioMX1Xg1n"
      },
      "source": [
        "The dataset has 60,000 images which can be used to train the model. There is also an additonal test set of 10,000 images which can be created by passing train=False to the MNIST class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SakVRGJIXh6N"
      },
      "source": [
        "test_dataset = MNIST(root='MNIST/', train=False)\n",
        "print(len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_E2fLCqX-20"
      },
      "source": [
        "It's a pair, consisting of a 28x28 image and a label. The image is an object of the class PIL.Image.Image, which is a part of the Python imaging library Pillow. We can view the image within Jupyter using matplotlib, the de-facto plotting and graphing library for data science in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE8PHca1YCrH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbd4x2JiYV_J"
      },
      "source": [
        "Along with importing matplotlib, a special statement %matplotlib inline is added to indicate to Jupyter that we want to plot the graphs within the notebook. Without this line, Jupyter will show the image in a popup. Statements starting with % are called IPython magic commands, and are used to configure the behavior of Jupyter itself. You can find a full list of magic commands here: https://ipython.readthedocs.io/en/stable/interactive/magics.html ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqqT2RtgYY6e"
      },
      "source": [
        "Let's look at a couple of images from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2QnAWD3YaFa"
      },
      "source": [
        "image, label = dataset[2]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label: ',label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb4YjFo7Y5QJ"
      },
      "source": [
        "It's evident that these images are quite small in size, and recognizing the digits can sometimes be hard even for the human eye. While it's useful to look at these images, there's just one problem here: PyTorch doesn't know how to work with images. We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1gnttewY8L-"
      },
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4uQaSymZGft"
      },
      "source": [
        "PyTorch datasets allow us to specify one or more transformation functions which are applied to the images as they are loaded. torchvision.transforms contains many such predefined functions, and we'll use the ToTensor transform to convert images into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjH0LraHZOOZ"
      },
      "source": [
        "# download MNIST data in tensor format\n",
        "dataset = MNIST(root='MNIST/', \n",
        "                train=True, \n",
        "                transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0esso3pZmyE"
      },
      "source": [
        "img_tensor, label = dataset[0]\n",
        "print(img_tensor.shape, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnhRefJjZ67j"
      },
      "source": [
        "The image is now converted to a 1x28x28 tensor. The first dimension is used to keep track of the color channels. Since images in the MNIST dataset are grayscale, there's just one channel. Other datasets have images with color, in which case there are 3 channels: red, green and blue (RGB). Let's look at some sample values inside the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lchtjt2Z9Q4"
      },
      "source": [
        "print(img_tensor[0,10:15,10:15])\n",
        "print(torch.max(img_tensor), torch.min(img_tensor))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GJ4goFKaSsq"
      },
      "source": [
        "The values range from 0 to 1, with 0 representing black, 1 white and the values in between different shades of grey. We can also plot the tensor as an image using plt.imshow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71lk8sg5aTik"
      },
      "source": [
        "# plot the image by passing the 28 x 28 matrix\n",
        "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXikjs5JbI4m"
      },
      "source": [
        "### 3.2.Training and Validation Datasets\n",
        "While building real world machine learning models, it is quite common to split the dataset into 3 parts:\n",
        "\n",
        "1. Training set - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n",
        "2. Validation set - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n",
        "3. Test set - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n",
        "In the MNIST dataset, there are 60,000 training images, and 10,000 test images. The test set is standardized so that different researchers can report the results of their models against the same set of images.\n",
        "\n",
        "Since there's no predefined validation set, we must manually split the 60,000 images into training and validation datasets. Let's set aside 10,000 randomly chosen images for validation. We can do this using the random_spilt method from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPa11Lt9brXu"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "trn_ds, val_ds = random_split(dataset, [50000, 10000])\n",
        "len(trn_ds), len(val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y-4gLZPcTdZ"
      },
      "source": [
        "We can now created data loaders to help us load the data in batches. We'll use a batch size of 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGG8bWeWcUZi"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trn_dl = DataLoader(trn_ds, batch_size, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvZPD1aSc3Y_"
      },
      "source": [
        "We set shuffle=True for the training dataloader, so that the batches generated in each epoch are different, and this randomization helps generalize & speed up the training process. On the other hand, since the validation dataloader is used only for evaluating the model, there is no need to shuffle the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV0VFWmR99DQ"
      },
      "source": [
        "### 3.3.Model\n",
        "Now that we have prepared our data loaders, we can define our model.\n",
        "\n",
        "- A logistic regression model is almost identical to a linear regression model i.e. there are weights and bias matrices, and the output is obtained using simple matrix operations (pred = x @ w.t() + b).\n",
        "\n",
        "- Just as we did with linear regression, we can use nn.Linear to create the model instead of defining and initializing the matrices manually.\n",
        "\n",
        "- Since nn.Linear expects the each training example to be a vector, each 1x28x28 image tensor needs to be flattened out into a vector of size 784 (28*28), before being passed into the model.\n",
        "\n",
        "- The output for each image is vector of size 10, with each element of the vector signifying the probability a particular target label (i.e. 0 to 9). The predicted label for an image is simply the one with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef5NkMQh-Zg0"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 28*28\n",
        "num_classes = 10\n",
        "\n",
        "# Logistic Regression Model\n",
        "model = nn.Linear(input_size, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO1feTOw-zN7"
      },
      "source": [
        "Of course, this model is a lot larger than our previous model, in terms of the number of parameters. Let's take a look at the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Yrgco5-y3k"
      },
      "source": [
        "print(model.weight.shape)\n",
        "model.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4KBd3eA_Low"
      },
      "source": [
        "print(model.bias.shape)\n",
        "model.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuvTw_G8_a7B"
      },
      "source": [
        "Although there are a total of 7850 parameters here, conceptually nothing has changed so far. Let's try and generate some outputs using our model. We'll take the first batch of 100 images from our dataset, and pass them into our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEkWPfWH_gAm"
      },
      "source": [
        "for images, labels in trn_dl: # images = feature values of each image\n",
        "  print(labels)\n",
        "  print(images.shape)\n",
        "  outputs = model(images)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV28f2YJBsuk"
      },
      "source": [
        "This leads to an error, because our input data does not have the right shape. Our images are of the shape 1x28x28, but we need them to be vectors of size 784 i.e. we need to flatten them out. We'll use the .reshape method of a tensor, which will allow us to efficiently 'view' each image as a flat vector, without really chaging the underlying data.\n",
        "\n",
        "To include this additional functionality within our model, we need to define a custom model, by extending the nn.Module class from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNTZQInhCAk4"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = xb.reshape(-1, 784)\n",
        "    out = self.linear(xb)\n",
        "    return out\n",
        "  \n",
        "model = MnistModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYqSulUfC8zk"
      },
      "source": [
        "Inside the __init__ constructor method, we instantiate the weights and biases using nn.Linear. And inside the forward method, which is invoked when we pass a batch of inputs to the model, we flatten out the input tensor, and then pass it into self.linear.\n",
        "\n",
        "xb.reshape(-1, 28*28) indicates to PyTorch that we want a view of the xb tensor with two dimensions, where the length along the 2nd dimension is 28*28 (i.e. 784). One argument to .reshape can be set to -1 (in this case the first dimension), to let PyTorch figure it out automatically based on the shape of the original tensor.\n",
        "\n",
        "Note that the model no longer has .weight and .bias attributes (as they are now inside the .linear attribute), but it does have a .parameters method which returns a list containing the weights and bias, and can be used by a PyTorch optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbUYyd6qDCIS"
      },
      "source": [
        "# print(model.linear.weight.shape, model.linear.bias.shape)\n",
        "list(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzVBrVxCDVAr"
      },
      "source": [
        "Our new custom model can be used in the exact same way as before. Let's see if it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CtJjrRuDUlv"
      },
      "source": [
        "for images, labels in trn_dl: # images = feature values of each image\n",
        "  outputs = model(images)\n",
        "  break\n",
        "\n",
        "print(outputs)\n",
        "print('outputs.shape : ', outputs.shape)\n",
        "# [128, 10]: 128 = # of training sets. \n",
        "# 128개의 이미지를 넣었을 때 10개 값을 변환.\n",
        "# 10개의 값은 각 10개 class에 대한 계산 값.\n",
        "# SoftMax 를 이용해 10개의 값이 전부 0 과 1 사이값이 되고, 합이 1이 되도록 조정.\n",
        "\n",
        "print('Sample outputs :\\n', outputs[:2].data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX94STkEvrW"
      },
      "source": [
        "For each of the 100 input images, we get 10 outputs, one for each class. As discussed earlier, we'd like these outputs to represent probabilities, but for that the elements of each output row must lie between 0 to 1 and add up to 1, which is clearly not the case here.\n",
        "\n",
        "To convert the output rows into probabilities, we use the softmax function, which has the following formula:\n",
        "\n",
        "softmax\n",
        "\n",
        "First we replace each element yi in an output row by e^yi, which makes all the elements positive, and then we divide each element by the sum of all elements to ensure that they add up to 1.\n",
        "\n",
        "While it's easy to implement the softmax function (you should try it!), we'll use the implementation that's provided within PyTorch, because it works well with multidimensional tensors (a list of output rows in our case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mivVWD7bE0os"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUrHlBtkE8pY"
      },
      "source": [
        "The softmax function is included in the torch.nn.functional package, and requires us to specify a dimension along which the softmax must be applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhOAM-FPE9c3"
      },
      "source": [
        "# Apply Softmax for each output row\n",
        "probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "# Look at sample probability\n",
        "print(\"Sample probability: \\n\", probs[:2].data)\n",
        "\n",
        "# Add\n",
        "print(\"Sum: \", torch.sum(probs[0]).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vogGfzSYF2Kv"
      },
      "source": [
        "Finally, we can determine the predicted label for each image by simply choosing the index of the element with the highest probability in each output row. This is done using torch.max, which returns the largest element and the index of the largest element along a particular dimension of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNaDOMYsF1zE"
      },
      "source": [
        "max_prob, preds = torch.max(probs, dim=1)\n",
        "print(preds)\n",
        "print(max_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VQUl95OGN66"
      },
      "source": [
        "The numbers printed above are the predicted labels for the first batch of training images. Let's compare them with the actual labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlc3rDKSGHHm"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxC80sk6GO0k"
      },
      "source": [
        "Clearly, the predicted and the actual labels are completely different. Obviously, that's because we have started with randomly initialized weights and biases. We need to train the model i.e. adjust the weights using gradient descent to make better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DaTxmDUGUV4"
      },
      "source": [
        "### 3.4.Evaluation Metric and Loss Function\n",
        "Just as with linear regression, we need a way to evaluate how well our model is performing. A natural way to do this would be to find the percentage of labels that were predicted correctly i.e. the accuracy of the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ttQswaGhRR"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzJr2I3OGk7M"
      },
      "source": [
        "The == performs an element-wise comparison of two tensors with the same shape, and returns a tensor of the same shape, containing 0s for unequal elements, and 1s for equal elements. Passing the result to torch.sum returns the number of labels that were predicted correctly. Finally, we divide by the total number of images to get the accuracy.\n",
        "\n",
        "Note that we don't need to apply softmax to the outputs, since it doesn't change the relative order of the results. This is because e^x is an increasing function i.e. if y1 > y2, then e^y1 > e^y2 and the same holds true after averaging out the values to get the softmax.\n",
        "\n",
        "Let's calculate the accuracy of the current model, on the first batch of data. Obviously, we expect it to be pretty bad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYDyuaCEGotS"
      },
      "source": [
        "accuracy(outputs, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LVP1rD3HB5P"
      },
      "source": [
        "While the accuracy is a great way for us (humans) to evaluate the model, it can't be used as a loss function for optimizing our model using gradient descent, for the following reasons:\n",
        "\n",
        "1. It's not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
        "\n",
        "2. It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements.\n",
        "\n",
        "Due to these reasons, accuracy is a great evaluation metric for classification, but not a good loss function. A commonly used loss function for classification problems is the cross entropy, which has the following formula:\n",
        "\n",
        "cross-entropy\n",
        "\n",
        "While it looks complicated, it's actually quite simple:\n",
        "\n",
        "- For each output row, pick the predicted probability for the correct label. E.g. if the predicted probabilities for an image are [0.1, 0.3, 0.2, ...] and the correct label is 1, we pick the corresponding element 0.3 and ignore the rest.\n",
        "\n",
        "- Then, take the logarithm of the picked probability. If the probability is high i.e. close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiply the result by -1, which results is a large postive value of the loss for poor predictions.\n",
        "\n",
        "- Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data.\n",
        "\n",
        "Unlike accuracy, cross-entropy is a continuous and differentiable function that also provides good feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). This makes it a good choice for the loss function.\n",
        "\n",
        "As you might expect, PyTorch provides an efficient and tensor-friendly implementation of cross entropy as part of the torch.nn.functional package. Moreover, it also performs softmax internally, so we can directly pass in the outputs of the model without converting them into probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zBueYuqHlW8"
      },
      "source": [
        "loss_fn = F.cross_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4qM3b_jHoHH"
      },
      "source": [
        "# Loss for current batch of data\n",
        "loss = loss_fn(outputs, labels)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azy0i_CgHzrZ"
      },
      "source": [
        "Since the cross entropy is the negative logarithm of the predicted probability of the correct label averaged over all training samples, one way to interpret the resulting number e.g. 2.23 is look at e^-2.23 which is around 0.1 as the predicted probability of the correct label, on average. Lower the loss, better the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMEJ3t-vH8Xi"
      },
      "source": [
        "### 3.5.Training the model\n",
        "Now that we have defined the data loaders, model, loss function and optimizer, we are ready to train the model. The training process is identical to linear regression, with the addition of a \"validation phase\" to evaluate the model in each epoch. Here's what it looks like in pseudocode:\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      # Training phase\n",
        "      for batch in train_loader:\n",
        "          # Generate predictions\n",
        "          # Calculate loss\n",
        "          # Compute gradients\n",
        "          # Update weights\n",
        "          # Reset gradients\n",
        "    \n",
        "      # Validation phase\n",
        "      for batch in val_loader:\n",
        "          # Generate predictions\n",
        "          # Calculate loss\n",
        "          # Calculate metrics (accuracy etc.)\n",
        "      # Calculate average validation loss & metrics\n",
        "      \n",
        "      # Log epoch, loss & metrics for inspection\n",
        "\n",
        "Some parts of the training loop are specific the specific problem we're solving (e.g. loss function, metrics etc.) whereas others are generic and can be applied to any deep learning problem. Let's impelment the problem-specific parts within our MnistModel class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-_tiifbSKxO"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "  def forward(self, xb):          \n",
        "        xb = xb.reshape(-1, 784) # Reshape the input data\n",
        "        out = self.linear(xb)   \n",
        "        return out\n",
        "\n",
        "  def training_step(self, batch):\n",
        "    images, labels = batch\n",
        "    out = self(images)                    # Generate Prediction\n",
        "    loss = F.cross_entropy(out, labels)   # Calculate Loss\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch):\n",
        "    images, labels = batch\n",
        "    out = self(images)                    # Generate Prediction\n",
        "    loss = F.cross_entropy(out, labels)   # Calculate Loass\n",
        "    acc = accuracy(out, labels)           # Calculate Accuracy\n",
        "    return {'val_loss': loss, 'val_acc': acc}\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    batch_losses = [x['val_loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "    batch_accs = [x['val_acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "  def epoch_end(self, epoch, result):\n",
        "    print(\"Epoch [{}], val_acc: {:.4f}, val_loss: {:.4f}\".format(epoch, result['val_acc'], result['val_loss']))\n",
        "\n",
        "\n",
        "model = MnistModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXRnfivBTF_w"
      },
      "source": [
        "out = self(images) \n",
        "- 'self' of a class in Python refers to the Object itself. It is similar to the following:\n",
        "\n",
        "    * model = MnistModel()\n",
        "    * out = model(images)\n",
        "\n",
        "- But since we cannot do the above inside a class, we use 'self(images)' as an equivalent to 'model(images)', where 'self' refers to the object itself. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZnQQw1YV9vT"
      },
      "source": [
        "* 'evaluate' function: perform the validation phase\n",
        "* 'fit' function: peform the entire training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwIStrVRWJNo"
      },
      "source": [
        "def evaluate(model, val_dl):\n",
        "  outputs = [model.validation_step(batch) for batch in val_dl]\n",
        "  return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, trn_dl, val_dl):\n",
        "  history = []\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "  for epoch in range(epochs):\n",
        "    # Training Phase\n",
        "    for batch in trn_dl:\n",
        "      loss = model.training_step(batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()        # update weights w.r.t to optimization function\n",
        "      optimizer.zero_grad()   # set grad to zero to avoid grad_accumulation.\n",
        "\n",
        "    # Validation Phase\n",
        "    result = evaluate(model, val_dl)\n",
        "    model.epoch_end(epoch, result)\n",
        "    history.append(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOjrQvjMYDiv"
      },
      "source": [
        "The 'fit' function records the validation loss and metric from each epoch and returns a history of the training process. This is useful for debuggin & visualizing the training process. Before we train the model, let's see how the model performs on the validation set with the initial set of randomly initialized weights & biases.\n",
        "\n",
        "Configurations like batch size, learning rate etc. need to picked in advance while training machine learning models, and are called hyperparameters. Picking the right hyperparameters is critical for training an accurate model within a reasonable amount of time, and is an active area of research and experimentation. Feel free to try different learning rates and see how it affects the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vftQBVWYyuT"
      },
      "source": [
        "result0 = evaluate(model, val_dl)\n",
        "result0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5BMHS58bkm-"
      },
      "source": [
        "The initial accuracy is around 10%, which is what one might expect from a randomly intialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly). Also note that we are using the .format method with the message string to print only the first four digits after the decimal point.\n",
        "\n",
        "We are now ready to train the model. Let's train for 5 epochs and look at the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTOma8W7blcA"
      },
      "source": [
        "history1 = fit(5, 0.001, model, trn_dl, val_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLVAc761cM9w"
      },
      "source": [
        "That's a great result! With just 5 epochs of training, our model has reached an accuracy of over 80% on the validation set. Let's see if we can improve that by training for a few more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NLM6ZlUcKUD"
      },
      "source": [
        "history2 = fit(5, 0.001, model, trn_dl, val_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ9qB0g_cX0O"
      },
      "source": [
        "history3 = fit(5, 0.001, model, trn_dl, val_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlrEYSeQcZ8e"
      },
      "source": [
        "history4 = fit(5, 0.001, model, trn_dl, val_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5P_YMkDcjws"
      },
      "source": [
        "While the accuracy does continue to increase as we train for more epochs, the improvements get smaller with every epoch. This is easier to see using a line graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05DdSTMsdwSe"
      },
      "source": [
        "### 3.6.Saving and loading the model\n",
        "Since we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights and bias matrices to disk, so that we can reuse the model later and avoid retraining from scratch. Here's how you can save the model.\n",
        "\n",
        "https://www.youtube.com/watch?v=g6kQl_EFn84"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k2aNJJ_gTO1"
      },
      "source": [
        "torch.save(model.state_dict(), 'mnist-logistic.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWXDlPnCgW9U"
      },
      "source": [
        "The .state_dict method returns an OrderedDict containing all the weights and bias matrices mapped to the right attributes of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWEH9YSOgcjf"
      },
      "source": [
        "To load the model weights, we can instante a new object of the class MnistModel, and use the .load_state_dict method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYMT1Oa3gdv8"
      },
      "source": [
        "model2 = MnistModel()\n",
        "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
        "model2.state_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDy38D0Bgpd5"
      },
      "source": [
        "# Define test dataset\n",
        "test_dataset = MNIST(root='MNIST/', \n",
        "                     train=False,\n",
        "                     transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HGaQ_a7guEy"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "result = evaluate(model2, test_loader)\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZid9_BuXvbs"
      },
      "source": [
        "## 4.Training DNN on a GPU with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLzl19TnbaLz"
      },
      "source": [
        "### 4.1. Preparing the Data\n",
        "- Import required modules and classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlkk1XTjX3AN"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cBnUERFboBx"
      },
      "source": [
        "dataset = MNIST(root='data/',\n",
        "                download=True,\n",
        "                transform=ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6PrrKs0c74t"
      },
      "source": [
        "# Use the random_split helper function to set aside 10000 images for our validation set.\n",
        "val_size = 10000\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzoLLs6-hljn"
      },
      "source": [
        "#### PyTorch DataLoader\n",
        "[reference]\n",
        "https://www.youtube.com/watch?v=zN49HdDxHi8\n",
        "* Create a dataloader which creates batches of data\n",
        "* one epoch: one forward pass and one backward pass of all the training examples.\n",
        "* batch size: the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
        "* number of iterations: number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass(we don't count the forward pass and backward pass as two different pass)\n",
        "\n",
        "* ex) if you have 1,000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bTWFyGJek5l"
      },
      "source": [
        "batch_size=128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-WxWTNHhqnD"
      },
      "source": [
        "# create a dataloader for training data\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "# create a dataloader for validation data\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLWaWfyRh3MB"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q2VP-DSnsEI"
      },
      "source": [
        "# Setting GPU Environmnet: RunTime > Change Runtime Type(런타임 유형 변경) > 하드웨어 가속기 > GPU\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kwHG_pMofco"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}